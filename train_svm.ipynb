{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def create_training_data(folder_path, classes, img_size):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for category in classes:\n",
    "        path = os.path.join(folder_path, category)\n",
    "        class_num = classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))\n",
    "                new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                X_train.append(new_array)\n",
    "                y_train.append(class_num)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    print(\"Converted image shape: \" + str(X_train[0].shape))\n",
    "    return(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted image shape: (48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data_path = \"data\"\n",
    "classes = [\"anger\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "\n",
    "X_train, y_train = create_training_data(train_data_path, classes, 48)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25409, 6912)\n"
     ]
    }
   ],
   "source": [
    "X_test = None\n",
    "y_test = None\n",
    "epochs = 2\n",
    "batch_size = 256\n",
    "\n",
    "number_of_labels = len(classes)\n",
    "# Aplatir (vectoriser) les images\n",
    "n_samples = len(X_train)\n",
    "X_train = X_train.reshape((n_samples, -1))\n",
    "input_shape = X_train.shape\n",
    "print(input_shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "REDUCTION DE DIMENSIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 150, whiten=True, random_state=42)\n",
    "pca_transformee = pca.fit_transform(X_train)\n",
    "\n",
    "# Enregistrer les donnees transformees dans un fichier csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pca_transformee)  # Créez un DataFrame avec les données transformées\n",
    "df.to_csv('donnees_transformees.csv', index=False)  # Enregistrez le DataFrame dans un fichier CSV\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CREATION DU MODELE SVM LINEAIRE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = svm.LinearSVC(max_iter=100000)\n",
    "# Création d'un classificateur calibré pour pouvoir tracer la courbe ROC_AUC\n",
    "calibrated_svc = CalibratedClassifierCV(model, method='sigmoid')\n",
    "pipeline= make_pipeline(pca_transformee,calibrated_svc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ENTRAINEMENT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [25409, 20327]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m----> 6\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining ...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   2557\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m   2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[0;32m   2563\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[0;32m   2564\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:443\u001B[0m, in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[0;32m    425\u001B[0m \n\u001B[0;32m    426\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    442\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[1;32m--> 443\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [25409, 20327]"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df,y_train,test_size=0.2,random_state=42)\n",
    "print(\"Training ...\")\n",
    "start_time = time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "end_time = time()\n",
    "duration = end_time-start_time\n",
    "\n",
    "print(\"Training duration: \", str(datetime.timedelta(seconds=duration)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "support_vectors = model.support_vectors_\n",
    "dec = model.decision_function([[1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "#\n",
    "# print(\"TEST RESULTS\")\n",
    "# y_test_model = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_test_model))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Obtenir les probabilités de prédiction pour les données de test\n",
    "y_pred_proba = calibrated_svc.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# from joblib import dump\n",
    "#\n",
    "# dump(model, 'model_svm.joblib')\n",
    "\n",
    "# later, the model can be retrieved with \"clf = load('filename.joblib')\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
